rm(list=ls())	#Begin with clean workspace.#
library(distr) #For igamma, the inverse of the gamma function.#
library(Matrix)#
set.seed(123)
data <- as.matrix(read.table('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt'),ncol=3,byrow=T)
plot(data)
dir_logl <- function(data,alpha){	#
	N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N*( log(gamma(sum(alpha))) - sum(log(gamma(alpha))) + sum((alpha-1)*logxi))#
	return(logl)	#
	#Note: Zeros are to prevent errors in case of log(0) values.#
}
conv <- 1*10^(-3)#
	maxiter=1000
loglik <- rep(0,maxiter)
#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
	#Initialize matrix to hold alphas for each iteration.#
	alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
	alphas[1,] <- rep(1,3)
N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N
logxi
loglik[i] <- N*( log(gamma(sum(alphas[i,]))) - #
			sum(log(gamma(alphas[i,]))) + sum((alphas[i,]-1)*logxi))
i=1
loglik[i] <- N*( log(gamma(sum(alphas[i,]))) - #
			sum(log(gamma(alphas[i,]))) + sum((alphas[i,]-1)*logxi))
loglik[1]
conv <- 1*10^(-5)#
	maxiter=1000#
	#1. Initialize values.#
	loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
	#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
	#Initialize matrix to hold alphas for each iteration.#
	alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
	alphas[1,] <- rep(1,3)#
#
	N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
	#Initialize log-likelihood.#
	loglik[i] <- N*( log(gamma(sum(alphas[i,]))) - #
			sum(log(gamma(alphas[i,]))) + sum((alphas[i,]-1)*logxi))
loglik[i]
for (i in 2:maxiter){#
		alphas[i,] <- igamma(logxi + digamma(sum(alphas[i-1,])))#
		loglik[i] <- dir_logl(data,alphas[i,])#
		if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
			print('Convergence reached.')#
			break;#
		}	#
	}
digamma(8)
igamma(digamma(8))
logxi <- colSums(log(data)) /N
logxi
colMeans(data)
colMeans(log(data))
dir_logl <- function(data,alpha){	#
	N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)#
	#logl <- N*( log(gamma(sum(alpha))) - sum(log(gamma(alpha))) + sum((alpha-1)*logxi))#
	return(logl)	#
	#Note: Zeros are to prevent errors in case of log(0) values.#
}
#Dirichlet gradient function:#
dir_gradient <- function(data,alpha,maxiter=5000,tol <- 1*10^(-5)){	#
	conv <- 1*10^(-5)#
	maxiter=1000#
	#1. Initialize values.#
	loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
	#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
	#Initialize matrix to hold alphas for each iteration.#
	alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
	alphas[1,] <- rep(1,3)#
#
	N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
	#Initialize log-likelihood.#
	loglik[i] <- dir_logl(data,alpha[1,])#
	for (i in 2:maxiter){#
		alphas[i,] <- igamma(logxi + digamma(sum(alphas[i-1,])))#
		loglik[i] <- dir_logl(data,alphas[i,])#
		if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
			print('Convergence reached.')#
			break;#
		}	#
	}
alphas[i,]
i=1
alpha[1,]
alphas[1,]
i=1
newton_roots <- function(y){#
  psiinv_y <- array(NA, dim=c(6,length(y)))#
  gamma <- -digamma(1)#
  for (i in 1:length(y)){#
    if (y[i] >= -2.22)#
      psiinv_y[1,i] <- exp(y[i]) + 0.5#
    else#
      psiinv_y[1,i] <- -1/(y[i]+gamma)#
  }#
  for (i in 2:6){#
    psiinv_y[i,] <- psiinv_y[i-1,] - (digamma(psiinv_y[i-1,]) - y)/(trigamma(psiinv_y[i-1,]))#
  }#
  return (psiinv_y[6,])#
}
conv <- 1*10^(-5)#
	maxiter=1000#
	#1. Initialize values.#
	loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
	#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
	#Initialize matrix to hold alphas for each iteration.#
	alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
	alphas[1,] <- rep(1,3)#
#
	N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
	#Initialize log-likelihood.#
	loglik[i] <- dir_logl(data,alpha[1,])#
	for (i in 2:maxiter){#
		alphas[i,] <- newton_roots(logxi + digamma(sum(alphas[i-1,])))#
		loglik[i] <- dir_logl(data,alphas[i,])#
		if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
			print('Convergence reached.')#
			break;#
		}	#
	}
alphas[i,]
class(data)
x1 <- x2 <- seq(.01,.99,by=.01)#
z <- outer(x1,x2,f,alpha_hat$alpha_hat)
?outer
f <- function(x1,x2,alpha){#
	f <- (sum(alpha) / sum(gamma(alpha)) ) * x1^(alpha[1]-1) * x2^(alpha[2]-1)#
}
f(0,1,c(1,2))
f
f <- function(x1,x2,alpha){#
	f <- (sum(alpha) / sum(gamma(alpha)) ) * x1^(alpha[1]-1) * x2^(alpha[2]-1)#
	return(f)#
}
f(0,1,c(1,2))
x1 <- x2 <- seq(.01,.99,by=.01)#
z <- outer(x1,x2,f,alpha_hat$alpha_hat)
alpha_hat <- alphas[i,]
alpha_hat
z <- outer(x1,x2,f,alpha_hat)
z
contour(x1,x2,z)
x1
x2
z
dim(z)
z <- lower(outer(x1,x2,f,alpha_hat))
z <- lower.tri(outer(x1,x2,f,alpha_hat),diag=T)
z
head(z)
z[1:10,1:10]
f <- function(x1,x2,alpha){#
	f <- ((gamma(sum(alpha)) / product(gamma(alpha)) ) #
		* x1^(alpha[1]-1) * x2^(alpha[2]-1) * (1-x1-x2)^(alpha[3]-1))#
	return(f)#
}
x1 <- x2 <- seq(.01,.99,by=.01)#
z <- outer(x1,x2,f,alpha_hat)
f <- function(x1,x2,alpha){#
	f <- ((gamma(sum(alpha)) / prod(gamma(alpha)) ) #
		* x1^(alpha[1]-1) * x2^(alpha[2]-1) * (1-x1-x2)^(alpha[3]-1))#
	return(f)#
}
?prod
x1 <- x2 <- seq(.01,.99,by=.01)#
z <- outer(x1,x2,f,alpha_hat)
z
head(z)
contour(z)
contour(z)#
points(data,col='red')
contour(z,col='red')#
points(data,col='black',pch=20)
#FIND MLE OF ALPHA VECTOR:#
	conv <- 1*10^(-5)#
	maxiter=1000#
	#1. Initialize values.#
	loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
	#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
	#Initialize matrix to hold alphas for each iteration.#
	alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
	alphas[1,] <- rep(1,3)#
#
	N <- nrow(data)							#Number of (j) observations.#
	logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
	#Initialize log-likelihood.#
	loglik[i] <- dir_logl(data,alpha[1,])#
	for (i in 2:maxiter){#
		alphas[i,] <- newton_roots(logxi + digamma(sum(alphas[i-1,])))#
		loglik[i] <- dir_logl(data,alphas[i,])#
		if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
			print('Convergence reached.')#
			break;#
		}	#
	}
alpha_hat
warnings()
#Inversion of digamma function#
inv_digamma <- function(y){#
  psiinv_y <- array(NA, dim=c(6,length(y)))#
  gamma <- -digamma(1)#
  for (i in 1:length(y)){#
    if (y[i] >= -2.22)#
      psiinv_y[1,i] <- exp(y[i]) + 0.5#
    else#
      psiinv_y[1,i] <- -1/(y[i]+gamma)#
  }#
  for (i in 2:6){#
    psiinv_y[i,] <- psiinv_y[i-1,] - (digamma(psiinv_y[i-1,]) - y)/(trigamma(psiinv_y[i-1,]))#
  }#
  return (psiinv_y[6,])#
}
#FIND MLE OF ALPHA VECTOR:#
conv <- 1*10^(-5)#
maxiter=1000#
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
#Initialize log-likelihood.#
loglik[i] <- dir_logl(data,alpha[1,])#
for (i in 2:maxiter){#
	alphas[i,] <- inv_digamma(logxi + digamma(sum(alphas[i-1,])))#
	loglik[i] <- dir_logl(data,alphas[i,])#
	if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
		print('Convergence reached.')#
		break;#
	}	#
}
#FIND MLE OF ALPHA VECTOR:#
conv <- 1*10^(-5)#
maxiter=1000#
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
#Initialize log-likelihood.#
loglik[i] <- dir_logl(data,alpha[1,])
alphaloglik[i] <- dir_logl(data,alphas[1,])
loglik[i] <- dir_logl(data,alphas[1,])
loglik[i]
rm(list=ls())	#Begin with clean workspace.#
library(distr) #For igamma, the inverse of the gamma function.#
library(Matrix)#
set.seed(123)
data <- as.matrix(read.table('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt'),ncol=3,byrow=T)#
#
#data <- scan('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt',what='numeric')#
#data <- as.numeric(data)#
#
plot(data)
#Dirichlet Log-likelihood function:#
dir_logl <- function(data,alpha){	#
	N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)#
#
	return(logl)	#
}#
#
#------------------#
#Inversion of digamma function#
inv_digamma <- function(y){#
  psiinv_y <- array(NA, dim=c(6,length(y)))#
  gamma <- -digamma(1)#
  for (i in 1:length(y)){#
    if (y[i] >= -2.22)#
      psiinv_y[1,i] <- exp(y[i]) + 0.5#
    else#
      psiinv_y[1,i] <- -1/(y[i]+gamma)#
  }#
  for (i in 2:6){#
    psiinv_y[i,] <- psiinv_y[i-1,] - (digamma(psiinv_y[i-1,]) - y)/(trigamma(psiinv_y[i-1,]))#
  }#
  return (psiinv_y[6,])#
}
conv <- 1*10^(-5)#
maxiter=1000#
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
#Initialize log-likelihood.#
loglik[i] <- dir_logl(data,alphas[1,])
#Initialize log-likelihood.#
loglik[1] <- dir_logl(data,alphas[1,])
alphas[1,]
N <- nrow(data)
N
logxi <- colMeans(log(data))
logxi
logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)
alpha <- alphas[1,]
logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)
logl
rm(list=ls())	#Begin with clean workspace.#
library(distr) #For igamma, the inverse of the gamma function.#
library(Matrix)#
set.seed(123)#
#
#2-c-ii:  #
#
n <- 100#
mu <- 5#
sd <- 1#
#
y <- rnorm(n,mu,sd)#
#
mu_hat <- mean(y)#
theta_hat <- exp(mu_hat)#
#
#Parametric bootstrap for var(theta).#
B <- 10000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrapped theta estimates.#
#
for (i in 1:B){#
	y_boot <- sample(y,n,replace=T)#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
	#theta_boot[i] <- exp(mean(y_boot))#
}#
var_theta_boot <- var(theta_boot)#
#
#Display var(theta):#
var_theta_boot#
#
#Display 95% CI: (Using normal interval)#
theta_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)#
#
#------------------------------------------------------------------------------#
#
#3c:#
#
#------------------------------------------------------------------------------#
#i: Gives a scatter plot of the data.#
data <- as.matrix(read.table('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt'),ncol=3,byrow=T)#
#
#data <- scan('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt',what='numeric')#
#data <- as.numeric(data)#
#
plot(data)#
#------------------------------------------------------------------------------#
#ii: Compute the MLE alpha_hat, and plot the log-likelihood as a function of iteration.#
#Briefly give a description of the algorithm you use.#
#
#------------------#
#Dirichlet Log-likelihood function:#
dir_logl <- function(data,alpha){	#
	N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)#
#
	return(logl)	#
}#
#
#------------------#
#Inversion of digamma function#
inv_digamma <- function(y){#
  psiinv_y <- array(NA, dim=c(6,length(y)))#
  gamma <- -digamma(1)#
  for (i in 1:length(y)){#
    if (y[i] >= -2.22)#
      psiinv_y[1,i] <- exp(y[i]) + 0.5#
    else#
      psiinv_y[1,i] <- -1/(y[i]+gamma)#
  }#
  for (i in 2:6){#
    psiinv_y[i,] <- psiinv_y[i-1,] - (digamma(psiinv_y[i-1,]) - y)/(trigamma(psiinv_y[i-1,]))#
  }#
  return (psiinv_y[6,])#
}
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N
#FIND MLE OF ALPHA VECTOR:#
conv <- 1*10^(-5)#
maxiter=1000#
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
#Initialize log-likelihood.#
loglik[1] <- dir_logl(data,alphas[1,])
dir_logl(data,alphas[1,])
alpha <- c(1,1,1)
alpha
N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum(alpha-1)*logxi)
logl
logxi
#------------------#
#Dirichlet Log-likelihood function:#
dir_logl <- function(data,alpha){	#
	N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	#Log-likelihood function.#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum((alpha-1)*logxi))#
#
	return(logl)	#
}
dir_logl(data,alpha)
#FIND MLE OF ALPHA VECTOR:#
conv <- 1*10^(-5)#
maxiter=1000#
#1. Initialize values.#
loglik <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood function.#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(data)) 	#
#Initialize matrix to hold alphas for each iteration.#
alphas <- matrix(0,nrow=maxiter+1,ncol=ncol(data)) 		#
alphas[1,] <- rep(1,3)#
#
N <- nrow(data)							#Number of (j) observations.#
logxi <- colSums(log(data)) /N		#logxi = (1/N)*sum(log(x_ij)#
#Initialize log-likelihood.#
loglik[1] <- dir_logl(data,alphas[1,])#
for (i in 2:maxiter){#
	alphas[i,] <- inv_digamma(logxi + digamma(sum(alphas[i-1,])))#
	loglik[i] <- dir_logl(data,alphas[i,])#
	if (abs((loglik[i]-loglik[i-1]))/abs(loglik[i-1]+1E-3) < conv){#
		print('Convergence reached.')#
		break;#
	}	#
}#
alpha_hat <- alphas[i,]
alpha_hat
f <- function(x1,x2,alpha){#
	f <- ((gamma(sum(alpha)) / prod(gamma(alpha)) ) #
		* x1^(alpha[1]-1) * x2^(alpha[2]-1) * (1-x1-x2)^(alpha[3]-1))#
	return(f)#
}#
#
x1 <- x2 <- seq(.01,.99,by=.01)#
z <- outer(x1,x2,f,alpha_hat)#
#
contour(z,col='red')#
points(data,col='black',pch=20)
### PART i: Gives a scatter plot of the data.#
data <- as.matrix(read.table('/Users/jennstarling/UTAustin/2016_Fall_SDS 383C_Statistical Modeling 1/Homework/HW 01/dir1.txt'),ncol=3,byrow=T)#
#
plot(data) #Is a 2-simplex.
plot(1:i,loglik)
length(loglik)
i
plot(1:i,loglik[1:i],type='l',col='blue')
plot(1:i,loglik[1:i],type='l',col='blue',xlab='iteration',ylab='log-likelihood',#
	main='Dirichlet Log-Likelihood Convergence')
alpha_hat
plot(1:i,loglik[1:i],type='l',col='blue',xlab='iteration',ylab='log-likelihood',#
	main='Dirichlet Log-Likelihood Convergence')
contour(z,col='red')#
points(data,col='black',pch=20)
contour(z,col='red',main='Dirichlet with alpha_hat Parameters & Data')#
points(data,col='black',pch=20)
rm(list=ls())	#Cleans workspace.#
library(microbenchmark)#
library(permute)#
#
#PART C:#
#
#Read in code.#
wdbc = read.csv('/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Stats Models for Big Data/Course Data/wdbc.csv', header=FALSE)#
y = wdbc[,2]#
#
#Convert y values to 1/0's.#
Y = rep(0,length(y)); Y[y=='M']=1#
X = as.matrix(wdbc[,-c(1,2)])#
#
#Select features to keep, and scale features.#
scrub = which(1:ncol(X) %% 3 == 0)#
scrub = 11:30#
X = X[,-scrub]#
X <- scale(X) #Normalize design matrix features.#
X = cbind(rep(1,nrow(X)),X)#
#
#Set up vector of sample sizes.  (All 1 for wdbc data.)#
m <- rep(1,nrow(X))	#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+.01) + (m-Y)*log(1-w+.01)) #Calculate log-likelihood.#
		#Adding .01 to resolve issues with probabilities near 0 or 1.	#
	return(logl)	#
}#
#
#------------------------------------------------------------------#
#Function for calculating Euclidean norm of a vector.#
norm_vec <- function(x) sqrt(sum(x^2)) #
#
#------------------------------------------------------------------#
#Stochastic Gradient Function: #
	#Inputs: Vector X (One row of design matrix), vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of gradient function for binomial #
	#   logistic regression.#
#
gradient <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	return(gradient)#
}#
#
#------------------------------------------------------------------#
#Robbins-Monro Step Size Function:#
#	Inputs: C>0, a constant.  a in [.5,1], a constant.#
#		t, the current iteration number.  t0, the prior number of steps. #
#		(Try smallish t0, 1 to 2.)#
#	Outputs: step, the step size.#
#
rm_step <- function(C,a,t,t0){#
	step <- C*(t+t0)^(-a)#
	return(step)#
}#
#
#Playing with step sizes:#
t <- 1:50000#
sp <- rm_step(C=5,a=.75,t=t,t0=2)#
plot(t,sp)#
#
#Varying C:#
cl <- rainbow(5)#
#plot(t,rm_step(C,a[1],t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying a:#
#plot(t,rm_step(C[1],a,t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying t:#
cl2 <- rainbow(2)#
#plot(t,rm_step(C[2],a[5],t,t0),col=cl2,lwd=1,pch=20,cex=.5)#
#
#Play with ideal step size curve shape:#
C=2;  t0=2;  a=.5;#
#plot(t,rm_step(C,a,t,t0),type='l',col='blue')#
#
#------------------------------------------------------------------#
#Stochastic Gradient Descent Algorithm:#
#
#1. Fit glm model for comparison. (No intercept: already added to X.)#
glm1 = glm(y~X-1, family='binomial') #Fits model, obtains beta values.#
beta <- glm1$coefficients#
#
maxiter <- 1000 	#Specify max iterations allowed.#
#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
#
#Initialize matrix to hold beta vector for each iteration.#
betas <- matrix(0,nrow=maxiter+1,ncol=ncol(X)) 	#
#
loglik <- rep(0,maxiter)		#Initialize vector to hold full loglikelihood fctn for each iter.#
loglik_t <- rep(0,maxiter)		#Initialize vector to hold loglikelihood for each indiv t obs.#
loglik_ra <- rep(0,maxiter)		#Initialize vector to hold running avg for logl for t's.#
#
iter <- 0 		    #Track number of iterations until convergence..#
conv <- 1*10^-12	#Set convergence level.#
#
betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
#
#Set up random iterations through data, up to maxiter.#
npermutes <- ceiling(maxiter/nrow(X))#
obs_order <- as.vector(t(shuffleSet(1:nrow(X),nset=npermutes)))
maxiter=100000
#Playing with step sizes:#
t <- 1:50000#
sp <- rm_step(C=5,a=.75,t=t,t0=2)#
plot(t,sp)#
#
#Varying C:#
cl <- rainbow(5)#
#plot(t,rm_step(C,a[1],t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying a:#
#plot(t,rm_step(C[1],a,t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying t:#
cl2 <- rainbow(2)#
#plot(t,rm_step(C[2],a[5],t,t0),col=cl2,lwd=1,pch=20,cex=.5)#
#
#Play with ideal step size curve shape:#
C=5;  t0=2;  a=.75;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=5;  t0=2;  a=.75;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=5;  t0=2;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=5;  t0=.5;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#Playing with step sizes:#
t <- 1:5000#
sp <- rm_step(C=5,a=.75,t=t,t0=2)#
plot(t,sp)#
#
#Varying C:#
cl <- rainbow(5)#
#plot(t,rm_step(C,a[1],t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying a:#
#plot(t,rm_step(C[1],a,t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying t:#
cl2 <- rainbow(2)#
#plot(t,rm_step(C[2],a[5],t,t0),col=cl2,lwd=1,pch=20,cex=.5)#
#
#Play with ideal step size curve shape:#
C=5;  t0=.5;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
t <- 1:50#
#sp <- rm_step(C=5,a=.75,t=t,t0=2)#
p#lot(t,sp)#
#
#Varying C:#
cl <- rainbow(5)#
#plot(t,rm_step(C,a[1],t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying a:#
#plot(t,rm_step(C[1],a,t,t0[2]),col=cl,lwd=1,pch=20,cex=.5)#
#
#Varying t:#
cl2 <- rainbow(2)#
#plot(t,rm_step(C[2],a[5],t,t0),col=cl2,lwd=1,pch=20,cex=.5)#
#
#Play with ideal step size curve shape:#
C=5;  t0=.5;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#Play with ideal step size curve shape:#
C=5;  t0=.75;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=10;  t0=.75;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#Play with ideal step size curve shape:#
C=10;  t0=.5;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#Play with ideal step size curve shape:#
C=200;  t0=.5;  a=1;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=5;  t0=1;  a=.5;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#Play with ideal step size curve shape:#
C=10;  t0=1;  a=.5;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
C=10;  t0=1;  a=.75;#
plot(t,rm_step(C,a,t,t0),type='l',col='blue')
#1. Fit glm model for comparison. (No intercept: already added to X.)#
glm1 = glm(y~X-1, family='binomial') #Fits model, obtains beta values.#
beta <- glm1$coefficients#
#
maxiter <- 200000 	#Specify max iterations allowed.#
#
#Initialize matrix to hold gradients for each iteration.					#
grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
#
#Initialize matrix to hold beta vector for each iteration.#
betas <- matrix(0,nrow=maxiter+1,ncol=ncol(X)) 	#
#
loglik <- rep(0,maxiter)		#Initialize vector to hold full loglikelihood fctn for each iter.#
loglik_t <- rep(0,maxiter)		#Initialize vector to hold loglikelihood for each indiv t obs.#
loglik_ra <- rep(0,maxiter)		#Initialize vector to hold running avg for logl for t's.#
#
iter <- 0 		    #Track number of iterations until convergence..#
conv <- 1*10^-12	#Set convergence level.#
#
betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
#
#Set up random iterations through data, up to maxiter.#
npermutes <- ceiling(maxiter/nrow(X))#
obs_order <- as.vector(t(shuffleSet(1:nrow(X),nset=npermutes)))#
#
#2. Perform stoachstic gradient descent.#
for (i in 1:maxiter){#
		#t <- sample(1:nrow(X),1)		#Select one random observation per iteration.#
		t  <- obs_order[i]#
		Xnew <- matrix(X[t,,drop=F],nrow=1,byrow=T)#
		#Calculate fullloglikelihood for each iteration.#
		loglik[i] <- logl(X,Y,betas[i,],m)#
		#Calculate loglikelihood of individual observation t.#
		loglik_t[i] <- logl(Xnew,Y[t],betas[i,],m[t])#
		#Calculate running average of loglikelihood for individual t's.#
		loglik_ra[i] <- mean(loglik_t[1:i]) #
		#Calculate stochastic gradient for beta, using only obs t.#
		grad[i,] <- gradient(Xnew,Y[t],betas[i,],m[t])#
		#Calculate Robbins-Monro step size.#
		step <- rm_step(C=5,a=.75,t=i,t0=2)#
		#step <- rm_step(C=2,a=.5,t=i,t0=2)#
		#step=.05#
		#Set new beta equal to beta - a*gradient(beta).#
		betas[i+1,] <- betas[i,] - step * grad[i,]#
		iter <- i + 1	#Track iterations.#
		print(iter)#
	#Check if convergence met:  If yes, exit loop.#
	#Note: Not using norm(gradient) like with regular gradient descent.#
	#Gradient is too variable in stochastic case.#
	#Can run for set iterations, but here, checking for convergence based#
	#on iter over iter change in running avg of log-likelihoods.#
	#Check if convergence met: If yes, exit loop.#
	if(i>1) {#
		if(abs(loglik_ra[i]-loglik_ra[i-1]) < conv) {#
			print('Algorithm has converged.')#
			break;#
		}	#
	}#
} #End gradient descent iterations.#
#
beta #GLM estimates#
betas[iter,] #Stochastic estimates.#
abs(loglik_ra[i]-loglik_ra[i-1])#
#
start <- 1#
par(mfrow=c(2,1))#
plot(start:(iter-1),loglik_ra[start:(iter-1)],type='l')#
plot(start:(iter-1),loglik[start:(iter-1)],type='l')#
par(mfrow=c(3,4))#
#Plot each beta vs GLM beta#
for (j in 1:length(beta)){#
	plot(1:iter,betas[1:iter,j],type='l')#
	abline(h=beta[j],col='blue')#
}
x <- seq(-1,2,by=.01)
y=x^2
plot(x,y,type='l')
x <- rnorm(100,mean=5,sd=1)#
y <- replicate(3000,exp(mean(rnorm(2000,mean=mean(x),sd=1))))#
z <- sqrt(sum((y-exp(mean(x)))^2)/length(y))#
w=mean(y)
w
n <- 100#
mu <- 5#
sd <- 1#
#
y <- rnorm(n,mu,sd)#
#
mu_hat <- mean(y)#
theta_hat <- exp(mu_hat)#
#
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
#Perform bootstrap to obtain vector of theta estimates.#
for (i in 1:B){#
	y_boot <- sample(y,n,replace=T)#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
	#theta_boot[i] <- exp(mean(y_boot))#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot
mean(mu)boot
mean(mu_boot)
x <- rnorm(100,mean=5,sd=1)#
y <- replicate(3000,exp(mean(rnorm(2000,mean=mean(x),sd=1))))
y
#Function for calculating Euclidean norm of a vector.#
norm_vec <- function(x) sqrt(sum(x^2))
beta
glm1 = glm(y~X-1, family='binomial') #Fits model, obtains beta values.#
beta <- glm1$coefficients
library(microbenchmark)#
library(permute)#
#
#Read in code.#
wdbc = read.csv('/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Course Data/wdbc.csv', header=FALSE)#
y = wdbc[,2]#
#
#Convert y values to 1/0's.#
Y = rep(0,length(y)); Y[y=='M']=1#
X = as.matrix(wdbc[,-c(1,2)],sparse=T)#
#
#Select features to keep, and scale features.#
scrub = which(1:ncol(X) %% 3 == 0)#
scrub = 11:30#
X = X[,-scrub]#
X <- scale(X) #Normalize design matrix features.#
X = cbind(rep(1,nrow(X)),X)#
#
#Set up vector of sample sizes.  (All 1 for wdbc data.)#
m <- rep(1,nrow(X))	#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	return(logl)	#
}#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function with l2 regularizer. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl_reg <- function(X,Y,beta,m,lambda){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	logl_reg <- logl + #
	return(logl)	#
}#
#
#------------------------------------------------------------------#
#Stochastic Gradient Function: #
	#Inputs: Vector X (One row of design matrix), vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of gradient function for binomial #
	#   logistic regression.#
#
gradient <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	return(gradient)#
}#
#
#------------------------------------------------------------------#
#AdaGrad Stochastic Gradient Descent Algorithm:#
#
	#Inputs:#
		#step = master step size.
beta
library(microbenchmark)#
library(permute)#
#
#Read in code.#
wdbc = read.csv('/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Course Data/wdbc.csv', header=FALSE)#
y = wdbc[,2]#
#
#Convert y values to 1/0's.#
Y = rep(0,length(y)); Y[y=='M']=1#
X = as.matrix(wdbc[,-c(1,2)],sparse=T)#
#
#Select features to keep, and scale features.#
scrub = which(1:ncol(X) %% 3 == 0)#
scrub = 11:30#
X = X[,-scrub]#
X <- scale(X) #Normalize design matrix features.#
X = cbind(rep(1,nrow(X)),X)#
#
#Set up vector of sample sizes.  (All 1 for wdbc data.)#
m <- rep(1,nrow(X))	#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	return(logl)	#
}#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function with l2 regularizer. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl_reg <- function(X,Y,beta,m,lambda){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	logl_reg <- logl + #
	return(logl)	#
}#
#
#------------------------------------------------------------------#
#Stochastic Gradient Function: #
	#Inputs: Vector X (One row of design matrix), vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#Output: Returns value of gradient function for binomial #
	#   logistic regression.#
#
gradient <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	return(gradient)#
}#
#
#------------------------------------------------------------------#
#AdaGrad Stochastic Gradient Descent Algorithm:#
#
	#Inputs:#
		#step = master step size.
glm1 = glm(y~X-1, family='binomial') #Fits model, obtains beta values.#
beta <- glm1$coefficients
beta
norm_vec(beta)
norm_vec(beta)^2
t(beta) %*% beta
sum(beta^2)
beta
I <- diag(1,nrow=length(beta))
I
dim(I)
length(beta)
t(beta) %*% I %*% beta
beta %*% I %*% t(beta)
logl_L2reg <- function(X,Y,beta,m,lambda){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	logl_reg <- logl + lambda*sum(beta^2)#
	return(logl)	#
}
logl(X,Y,beta,m)
logl_L2reg(X,Y,beta,m,1)
logl_L2reg(X,Y,beta,m,1.5)
logl_L2reg(X,Y,beta,m,10)
beta
sum(beta^2)
logl_L2reg <- function(X,Y,beta,m,lambda){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	logl_reg <- logl + lambda*sum(beta^2)#
	return(logl_reg)	#
}
logl(X,Y,beta,m)
logl_L2reg(X,Y,beta,m,0)
logl_L2reg(X,Y,beta,m,1)
logl_L2reg(X,Y,beta,m,.1)
gradient <- function(X,Y,beta,m){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	return(gradient)#
}#
#
#------------------------------------------------------------------#
#Gradient Function with L2 regularization.: #
	#Inputs: Vector X (One row of design matrix), vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#  lambda = scalar penalty for L2 regularization.  lambda > 0 required.#
	#Output: Returns value of gradient function for binomial #
	#   logistic regression.#
#
gradient_L2reg <- function(X,Y,beta,m,lambda){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	gradient_reg <- gradient + lambda*2*beta#
	return(gradient_reg)#
}
gradient(X,Y,beta,m)
gradient_L2reg(X,Y,beta,m,0)
gradient_L2reg(X,Y,beta,m,.1)
gradient_L2reg(X,Y,beta,m,1)
### SDS 385 - Exercises 04 - Part B#
#This code implements stochastic gradient descent to estimate the #
#beta coefficients for binomial logistic regression.#
#Step size is calculated using backtracking line search, and is tuned#
#using the adagrad algorithm.#
#
#Added:#
#L2 regularization to log-likelihood and gradient.#
#
#Jennifer Starling#
#18 September 2016#
#
rm(list=ls())	#Cleans workspace.#
library(microbenchmark)#
library(permute)#
#
#Read in code.#
wdbc = read.csv('/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Course Data/wdbc.csv', header=FALSE)#
y = wdbc[,2]#
#
#Convert y values to 1/0's.#
Y = rep(0,length(y)); Y[y=='M']=1#
X = as.matrix(wdbc[,-c(1,2)],sparse=T)#
#
#Select features to keep, and scale features.#
scrub = which(1:ncol(X) %% 3 == 0)#
scrub = 11:30#
X = X[,-scrub]#
X <- scale(X) #Normalize design matrix features.#
X = cbind(rep(1,nrow(X)),X)#
#
#Set up vector of sample sizes.  (All 1 for wdbc data.)#
m <- rep(1,nrow(X))	#
#
#------------------------------------------------------------------#
#Binomial Negative Loglikelihood function with l2 regularizer. #
	#Inputs: Design matrix X, vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#  lambda = scalar penalty for L2 regularization.  lambda > 0 required.#
	#Output: Returns value of negative log-likelihood #
	#   function for binomial logistic regression.#
logl <- function(X,Y,beta,m,lambda=0){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	logl <- - sum(Y*log(w+1E-4) + (m-Y)*log(1-w+1E-4)) #Calculate log-likelihood.#
		#Adding constant to resolve issues with probabilities near 0 or 1.	#
	logl_reg <- logl + lambda*sum(beta^2)#
	return(logl_reg)	#
}#
#
#------------------------------------------------------------------#
#Gradient Function with L2 regularization.: #
	#Inputs: Vector X (One row of design matrix), vector of 1/0 vals Y, #
	#   coefficient matrix beta, sample size vector m.#
	#  lambda = scalar penalty for L2 regularization.  lambda > 0 required.#
	#Output: Returns value of gradient function for binomial #
	#   logistic regression.#
#
gradient <- function(X,Y,beta,m,lambda=0){#
	w <- 1 / (1 + exp(-X %*% beta))	#Calculate probabilities vector w_i.#
	gradient <- array(NA,dim=length(beta))	#Initialize the gradient.#
	gradient <- apply(X*as.numeric(m*w-Y),2,sum) #Calculate the gradient.#
	gradient_reg <- gradient + lambda*2*beta#
	return(gradient_reg)#
}#
#
#------------------------------------------------------------------#
#AdaGrad Stochastic Gradient Descent Algorithm:#
#
	#Inputs:#
		#step = master step size.#
		#lambda = Scalar penalty for L2 regularization; must be lambda>0.#
sgd_adagrad <- function(X,Y,m,step=.01,maxiter=50000,conv=1E-10,lambda=0){#
#
	converged <- 0 #Indicator variable to track convergence status.#
	#Set up random iterations through data, up to maxiter.#
	npermutes <- ceiling(maxiter/nrow(X))#
	obs_order <- as.vector(t(shuffleSet(1:nrow(X),nset=npermutes)))#
	#Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X))#
	#Initialize vectors to hold Adagrad historical and adjusted gradients.#
	hist_grad <- rep(0,ncol(X))#
	adj_grad <- rep(0,ncol(X))#
	#Initialize constant for numerical stability in Adagrad calculations.#
	epsilon <- 1E-6 		#
#
	#Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter+1,ncol=ncol(X)) 	#
#
	#Initialize log-likelihood vectors.#
	loglik <- rep(0,maxiter,lambda)	#Full loglhood.#
	loglik_t <- rep(0,maxiter,lambda)	#Per-obs loglhood.	#
	loglik_ra <- rep(0,maxiter,lambda)	#Running avg of loglhoods.#
#
	#Initialize values for first iteration:#
	i=1#
	t  <- obs_order[i]#
	Xnew <- matrix(X[t,,drop=F],nrow=1,byrow=T)#
	loglik_t[i] <- logl(Xnew,Y[t],betas[i,],m[t],lambda)#
	loglik_ra[i] <- loglik_t[i]#
	grad[1,] <- gradient(Xnew,Y[t],betas[i,],m[t],lambda)#
	betas[1,] <- 0#
#
	#2. Perform stochastic gradient descent.#
	for (i in 2:maxiter){#
		#Select one random obs per iter.#
		t  <- obs_order[i]#
		Xnew <- matrix(X[t,,drop=F],nrow=1,byrow=T)#
		#Calculate updated AdaGrad historical and adjusted gradients.#
		hist_grad <- hist_grad + grad[i-1,]^2#
		adj_grad <- grad[i-1,] / (sqrt(hist_grad) + epsilon)#
		#Set new beta equal to beta - a*adj_grad(beta_i-1) where #
		#adj_grad is the AdaGrad adjusted gradient.#
		betas[i,] <- betas[i-1,] - step * adj_grad#
		#Calculate updated gradient for beta, using only obs t.#
		grad[i,] <- gradient(Xnew,Y[t],betas[i,],m[t],lambda)#
		#Calculate fullloglikelihood for each iteration.#
		loglik[i] <- logl(X,Y,betas[i,],m,lambda)#
		#Calculate loglikelihood of individual observation t.#
		loglik_t[i] <- logl(Xnew,Y[t],betas[i,],m[t],lambda)#
		#Calculate running average of loglikelihood for individual t's.#
		loglik_ra[i] <- (loglik_ra[i-1]*(i-1) + loglik_t[i])/i#
		print(i)#
		#Check if convergence met:  If yes, exit loop.#
		#Note: Not using norm(gradient) like with regular gradient descent.#
		#Gradient is too variable in stochastic case.#
		#Can run for set iterations, but here, checking for convergence based#
		#on iter over iter change in running avg of log-likelihoods.#
		#Check if convergence met: If yes, exit loop.#
		if (abs(loglik_ra[i]-loglik_ra[i-1])/abs(loglik_ra[i-1]+1E-3) < conv ){#
			converged=1;#
			break;#
		}#
	} #End gradient descent iterations.#
	#Return function output.#
	return(list(beta_hat=betas[i,],#
			betas=betas, #
			iter=i, #
			converged=converged, #
			loglik_full=loglik[1:i],#
			loglik_ra = loglik_ra[1:i],#
			loglik_indiv = loglik_t[1:i]))#
}#
#------------------------------------------------------------------#
#OUTPUT ANALYSIS:#
#
#1. Fit glm model for comparison. (No intercept: already added to X.)#
glm1 = glm(y~X-1, family='binomial') #Fits model, obtains beta values.#
beta <- glm1$coefficients#
#
#2. Run Adagrad algorithm.#
output <- sgd_adagrad(X,Y,m,step=5,maxiter=1000000,conv=1E-12)#
#
#3. Output beta estimates for glm vs adagrad.#
beta #GLM estimates#
output$beta_hat #Stochastic estimates.#
#
#4. Plog full loglhood, running avg loglhood, and convergence of beta estimates.#
#
#Plot full log-likelihood function for convergence, and running average for log-likelihoods.#
par(mfrow=c(2,1))#
plot(1:output$iter,output$loglik_full,type='l',xlab='i',ylab='full neg loglhood')#
plot(1:output$iter,output$loglik_ra,type='l',xlab='i',ylab='running avg neg loglhood')#
#
#Plot the convergence of the beta variables compared to glm.#
par(mfrow=c(4,3))#
for (j in 1:length(output$beta_hat)){#
	plot(1:nrow(output$betas),output$betas[,j],type='l',xlab='iterations',ylab=paste('beta',j))#
	abline(h=beta[j],col='red')#
}#
#------------------------------------------------------------------#
#SAVE PLOTS TO JPG FILES:#
#
#Save plots:#
jpeg(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 04 LaTeX Files/Ex04_adagrad_loglik.jpg')#
#
#Plot full log-likelihood function for convergence, and running average for log-likelihoods.#
par(mfrow=c(2,1))#
plot(1:output$iter,output$loglik_full,type='l',xlab='i',ylab='full neg loglhood')#
plot(1:output$iter,output$loglik_ra,type='l',xlab='i',ylab='running avg neg loglhood')#
dev.off()#
#
jpeg(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 04 LaTeX Files/Ex04_adagrad_betas.jpg')#
par(mfrow=c(4,3))#
for (j in 1:length(output$beta_hat)){#
	plot(1:nrow(output$betas),output$betas[,j],type='l',xlab='iterations',ylab=paste('beta',j))#
	abline(h=beta[j],col='red')#
}#
dev.off()
?quantile
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)
#Display 95% CI: (Using percentile interval)#
quantile(theta_hat,c(.025,.975))
quantile(th_hat,c(.025,.975))
th_hat
quantile(theta_boot,c(.025,.975))
#STATS MODELING 1 - 383C#
#5 Sept 2016#
#Jennifer Starling#
#Homework 01#
#
rm(list=ls())	#Begin with clean workspace.#
library(Matrix)#
set.seed(123)#
#
#######################################
##       PROBLEM 2-c-ii:            ###
#######################################
#
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot#
#
#Display 95% CI: (Using normal interval)#
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)#
#
#Display 95% CI: (Using percentile interval)#
quantile(theta_boot,c(.025,.975))
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot#
#
#Display 95% CI: (Using normal interval)#
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)#
#
#Display 95% CI: (Using percentile interval)#
quantile(theta_boot,c(.025,.975))
th_hat
rm(list=ls())	#Begin with clean workspace.#
library(Matrix)#
set.seed(123)#
#######################################
##       PROBLEM 2-c-ii:            ###
#######################################
#
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot#
#
#Display 95% CI: (Using normal interval)#
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)#
#
#Display 95% CI: (Using percentile interval)#
quantile(theta_boot,c(.025,.975))
e^10/100
exp(10)/100
rm(list=ls())	#Begin with clean workspace.#
library(Matrix)#
set.seed(123)#
#######################################
##       PROBLEM 2-c-ii:            ###
#######################################
#
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot#
#
#Display 95% CI: (Using normal interval)#
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)#
#
#Display 95% CI: (Using percentile interval)#
quantile(theta_boot,c(.025,.975))
setwd('/Users/jennstarling/UTAustin/starlib/data')#
#
#i: Gives a scatter plot of the data.#
data <- as.matrix(read.table('./dir1.txt'),ncol=3,byrow=T)#
#
plot(data,main='Scatter Plot of Data') #Is a 2-simplex.
library(starlib)
ls()
ls
starlib
setwd('/Users/jennstarling/UTAustin/starlib')
library(starlib)
inv.digamma
library(devtools)#
#
setwd("/Users/jennstarling/UT Austin/starlib")#
install("/Users/jennstarling/UT Austin/starlib") #
#
# Github varstar package install.#
#library(devtools)#
#install_github(‘jstarling1/starlib’,’jstarling1’)#
#
library(starlib)		#Load custom starlib package for all custom functions.#
data(package='starlib')	#View available starlib data sets.#
ls("package:starlib")	#View all functions in starlib package.
document()
ls()
document()
document()
setwd('/Users/jennstarling/UTAustin/starlib/data')
#Read data:#
data <- read.csv(file='./mpg_data.csv',header=T)
setwd('/Users/jennstarling/UTAustin/starlib/data/')
data <- read.csv(file='mpg_data.csv',header=T)
data <- read.csv(file='mpg_data.csv',header=T)
pdata <- read.table(file='prostate_data.txt')
setwd("/Users/jennstarling/UTAustin/starlib/data")
docs = read.table(file="./bbcsport.docs")#
#
words = read.table(file='./bbcsport.terms')
