### SDS 385 - Exercises 06 - Proximal Gradient Descent for LASSO.#
#
#Jennifer Starling#
#7 October 2016#
#
rm(list=ls())	#Clean workspace.#
#
library(glmnet)#
library(Matrix)#
#
#Read in Diabetes.csv data.#
X <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesX.csv',header=T)#
y <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesY.csv',header=F)#
#
#Scale X and y.#
X = scale(X)#
y = scale(y)#
#
#----------------------------------------------#
#LASSO objective function:#
#Inputs:#
#	X = X matrix (scaled)#
#	y = response data (scaled)#
#	lambda = a chosen lambda value#
#	beta = a vector of beta coefficients.#
#Output:#
#	Value of the LASSO objective function at specified inputs.#
fx <- function(X,y,lambda,beta){#
	f = (1/nrow(X)) * (t(y - X %*% beta) %*% (y - X %*% beta))#
	g = lambda * sum(abs(beta))#
	obj = (f+g)#
	return(as.numeric(obj))#
}#
#
#Test:#
fx(X,y,lam,beta_glmnet)#
#
#----------------------------------------------#
#Proximal L1 Operator function: (soft thresholding operator)#
#Inputs:#
#	x = vector of values.#
#	lambda = the scaling factor of the l1 norm.#
#	t = the step size.#
#
#Output:#
#	Value of the soft-thresholding proximal operator.#
#
#prox_l1 <- function(x,gamma,tau=1) {#
#	thresh <- gamma*tau#
#	prox = rep(0,length(x))#
#	idx.1 = which(x < -thresh)#
#	idx.2 = which(x > thresh)#
#	idx.3 = which(abs(x) <= thresh)#
#	if (length(idx.1) > 0) prox[idx.1] = x[idx.1] + thresh#
#	if (length(idx.2) > 0) prox[idx.2] = x[idx.2] - thresh#
#	if (length(idx.3) > 0) prox[idx.3] = 0#
#
 #   return(prox)#
#}#
prox_l1 <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
  # ----------------------------------------#
  # Args: #
  #   - x: vector of the observations#
  #   - lambda: penalization parameter (threshold)#
  # Returns: #
  #   - theta: the soft thresholding estimator#
  # ------------------------------------------#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
  return (theta)#
}#
#
#----------------------------------------------#
#Gradient for differentiable (non-penalty) part of LASSO objective:#
gradient <- function(X,y,beta){#
	grad = (2/nrow(X)) * (t(X) %*% X %*% beta - t(X) %*% y )#
	return(grad)#
}	#
#
#----------------------------------------------#
#Proximal Gradient Descent for L1 Norm Function:#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
proxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		#grad[i-1,] = (2/nrow(X)) * (t(X) %*% X %*% betas[i-1,]  - t(X) %*% y )#
		grad[i-1,] = gradient(X,y,betas[i-1,])#
		#Determine intermediate point.#
		z = betas[i-1,] - gamma*grad[i-1,]#
		#STEP 2: Proximal step.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-3) < tol ){#
			converged=1;#
			break;#
		}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i))#
} #end function#
#
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i-1,] + ((s[i-1]-1)/s[i]) * (betas[i-1,] - betas[i,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma*lambda)#
		#betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i-1,] + ((s[i-1]-1)/s[i]) * (betas[i-1,] - betas[i,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma*lambda)#
		#betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i-1,] + ((s[i-1]-1)/s[i]) * (betas[i,] - betas[i-1,])#
		#z[i,] = betas[i-1,] + ((s[i-1]-1)/s[i]) * (betas[i-1,] - betas[i,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)#
#
#Compare results to glmnet:#
myLasso <- glmnet(X,y,family='gaussian',lambda=lam)	#fit glmnet model.#
beta_glmnet <- myLasso$beta							#Save glmnet betas.#
cbind(glmnet=beta_glmnet,#
	proximal=output$beta_hat,#
	accel.prox=round(outputAccel$beta_hat,8)) 	#output comparison#
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
### SDS 385 - Exercises 06 - Proximal Gradient Descent for LASSO.#
#
#Jennifer Starling#
#7 October 2016#
#
rm(list=ls())	#Clean workspace.#
#
library(glmnet)#
library(Matrix)#
#
#Read in Diabetes.csv data.#
X <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesX.csv',header=T)#
y <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesY.csv',header=F)#
#
#Scale X and y.#
X = scale(X)#
y = scale(y)#
#
#----------------------------------------------#
#LASSO objective function:#
#Inputs:#
#	X = X matrix (scaled)#
#	y = response data (scaled)#
#	lambda = a chosen lambda value#
#	beta = a vector of beta coefficients.#
#Output:#
#	Value of the LASSO objective function at specified inputs.#
fx <- function(X,y,lambda,beta){#
	f = (1/nrow(X)) * (t(y - X %*% beta) %*% (y - X %*% beta))#
	g = lambda * sum(abs(beta))#
	obj = (f+g)#
	return(as.numeric(obj))#
}#
#
#Test:#
fx(X,y,lam,beta_glmnet)#
#
#----------------------------------------------#
#Proximal L1 Operator function: (soft thresholding operator)#
#Inputs:#
#	x = vector of values.#
#	lambda = the scaling factor of the l1 norm.#
#	t = the step size.#
#
#Output:#
#	Value of the soft-thresholding proximal operator.#
#
prox_l1 <- function(x,gamma,tau=1) {#
	thresh <- gamma*tau#
	prox = rep(0,length(x))#
	idx.1 = which(x < -thresh)#
	idx.2 = which(x > thresh)#
	idx.3 = which(abs(x) <= thresh)#
	if (length(idx.1) > 0) prox[idx.1] = x[idx.1] + thresh#
	if (length(idx.2) > 0) prox[idx.2] = x[idx.2] - thresh#
	if (length(idx.3) > 0) prox[idx.3] = 0#
#
    return(prox)#
}#
soft.thresh <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
  # ----------------------------------------#
  # Args: #
  #   - x: vector of the observations#
  #   - lambda: penalization parameter (threshold)#
  # Returns: #
  #   - theta: the soft thresholding estimator#
  # ------------------------------------------#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
  return (theta)#
}#
#
#----------------------------------------------#
#Gradient for differentiable (non-penalty) part of LASSO objective:#
gradient <- function(X,y,beta){#
	grad = (2/nrow(X)) * (t(X) %*% X %*% beta - t(X) %*% y )#
	return(grad)#
}	#
#
#----------------------------------------------#
#Proximal Gradient Descent for L1 Norm Function:#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
proxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		#grad[i-1,] = (2/nrow(X)) * (t(X) %*% X %*% betas[i-1,]  - t(X) %*% y )#
		grad[i-1,] = gradient(X,y,betas[i-1,])#
		#Determine intermediate point.#
		z = betas[i-1,] - gamma*grad[i-1,]#
		#STEP 2: Proximal step.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-3) < tol ){#
			converged=1;#
			break;#
		}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i))#
} #end function#
#
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i-1,] + ((s[i-1]-1)/s[i]) * (betas[i-1,] - betas[i,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)#
#
#Compare results to glmnet:#
myLasso <- glmnet(X,y,family='gaussian',lambda=lam)	#fit glmnet model.#
beta_glmnet <- myLasso$beta							#Save glmnet betas.#
cbind(glmnet=beta_glmnet,#
	proximal=output$beta_hat,#
	accel.prox=round(outputAccel$beta_hat,8)) 	#output comparison#
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
### SDS 385 - Exercises 06 - Proximal Gradient Descent for LASSO.#
#
#Jennifer Starling#
#7 October 2016#
#
rm(list=ls())	#Clean workspace.#
#
library(glmnet)#
library(Matrix)#
#
#Read in Diabetes.csv data.#
X <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesX.csv',header=T)#
y <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesY.csv',header=F)#
#
#Scale X and y.#
X = scale(X)#
y = scale(y)#
#
#----------------------------------------------#
#LASSO objective function:#
#Inputs:#
#	X = X matrix (scaled)#
#	y = response data (scaled)#
#	lambda = a chosen lambda value#
#	beta = a vector of beta coefficients.#
#Output:#
#	Value of the LASSO objective function at specified inputs.#
fx <- function(X,y,lambda,beta){#
	f = (1/nrow(X)) * (t(y - X %*% beta) %*% (y - X %*% beta))#
	g = lambda * sum(abs(beta))#
	obj = (f+g)#
	return(as.numeric(obj))#
}#
#
#Test:#
fx(X,y,lam,beta_glmnet)#
#
#----------------------------------------------#
#Proximal L1 Operator function: (soft thresholding operator)#
#Inputs:#
#	x = vector of values.#
#	lambda = the scaling factor of the l1 norm.#
#	t = the step size.#
#
#Output:#
#	Value of the soft-thresholding proximal operator.#
#
prox_l1 <- function(x,gamma,tau=1) {#
	thresh <- gamma*tau#
	prox = rep(0,length(x))#
	idx.1 = which(x < -thresh)#
	idx.2 = which(x > thresh)#
	idx.3 = which(abs(x) <= thresh)#
	if (length(idx.1) > 0) prox[idx.1] = x[idx.1] + thresh#
	if (length(idx.2) > 0) prox[idx.2] = x[idx.2] - thresh#
	if (length(idx.3) > 0) prox[idx.3] = 0#
#
    return(prox)#
}#
soft.thresh <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
  # ----------------------------------------#
  # Args: #
  #   - x: vector of the observations#
  #   - lambda: penalization parameter (threshold)#
  # Returns: #
  #   - theta: the soft thresholding estimator#
  # ------------------------------------------#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
  return (theta)#
}#
#
#----------------------------------------------#
#Gradient for differentiable (non-penalty) part of LASSO objective:#
gradient <- function(X,y,beta){#
	grad = (2/nrow(X)) * (t(X) %*% X %*% beta - t(X) %*% y )#
	return(grad)#
}	#
#
#----------------------------------------------#
#Proximal Gradient Descent for L1 Norm Function:#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
proxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		#grad[i-1,] = (2/nrow(X)) * (t(X) %*% X %*% betas[i-1,]  - t(X) %*% y )#
		grad[i-1,] = gradient(X,y,betas[i-1,])#
		#Determine intermediate point.#
		z = betas[i-1,] - gamma*grad[i-1,]#
		#STEP 2: Proximal step.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-3) < tol ){#
			converged=1;#
			break;#
		}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i))#
} #end function#
#
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i,] + ((s[i-1]-1)/s[i]) * (betas[i-1,] - betas[i,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #
	s = rep(0,maxiter)	#
	#Set up first z value.  (Used a regular gradient calculation for beta0.)#
	#z[1,] = betas[1,] - gamma * grad[1,]#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i,] + ((s[i-1]-1)/s[i]) * (betas[i,] - betas[i-1,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)#
#
#Compare results to glmnet:#
myLasso <- glmnet(X,y,family='gaussian',lambda=lam)	#fit glmnet model.#
beta_glmnet <- myLasso$beta							#Save glmnet betas.#
cbind(glmnet=beta_glmnet,#
	proximal=output$beta_hat,#
	accel.prox=round(outputAccel$beta_hat,8)) 	#output comparison#
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
rm(list=ls())	#Clean workspace.#
#
library(glmnet)#
library(Matrix)#
#
#Read in Diabetes.csv data.#
X <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesX.csv',header=T)#
y <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesY.csv',header=F)#
#
#Scale X and y.#
X = scale(X)#
y = scale(y)#
#
#----------------------------------------------#
#LASSO objective function:#
#Inputs:#
#	X = X matrix (scaled)#
#	y = response data (scaled)#
#	lambda = a chosen lambda value#
#	beta = a vector of beta coefficients.#
#Output:#
#	Value of the LASSO objective function at specified inputs.#
fx <- function(X,y,lambda,beta){#
	f = (1/nrow(X)) * (t(y - X %*% beta) %*% (y - X %*% beta))#
	g = lambda * sum(abs(beta))#
	obj = (f+g)#
	return(as.numeric(obj))#
}#
#
#Test:#
fx(X,y,lam,beta_glmnet)#
#
#----------------------------------------------#
#Proximal L1 Operator function: (soft thresholding operator)#
#Inputs:#
#	x = vector of values.#
#	lambda = the scaling factor of the l1 norm.#
#	t = the step size.#
#
#Output:#
#	Value of the soft-thresholding proximal operator.#
#
#prox_l1 <- function(x,gamma,tau=1) {#
#	thresh <- gamma*tau#
#	prox = rep(0,length(x))#
#	idx.1 = which(x < -thresh)#
#	idx.2 = which(x > thresh)#
#	idx.3 = which(abs(x) <= thresh)#
#	if (length(idx.1) > 0) prox[idx.1] = x[idx.1] + thresh#
#	if (length(idx.2) > 0) prox[idx.2] = x[idx.2] - thresh#
#	if (length(idx.3) > 0) prox[idx.3] = 0#
#
 #   return(prox)#
#}#
prox_l1 <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
  # ----------------------------------------#
  # Args: #
  #   - x: vector of the observations#
  #   - lambda: penalization parameter (threshold)#
  # Returns: #
  #   - theta: the soft thresholding estimator#
  # ------------------------------------------#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
  return (theta)#
}#
#
#----------------------------------------------#
#Gradient for differentiable (non-penalty) part of LASSO objective:#
gradient <- function(X,y,beta){#
	grad = (2/nrow(X)) * (t(X) %*% X %*% beta - t(X) %*% y )#
	return(grad)#
}	#
#
#----------------------------------------------#
#Proximal Gradient Descent for L1 Norm Function:#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
proxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		#grad[i-1,] = (2/nrow(X)) * (t(X) %*% X %*% betas[i-1,]  - t(X) %*% y )#
		grad[i-1,] = gradient(X,y,betas[i-1,])#
		#Determine intermediate point.#
		z = betas[i-1,] - gamma*grad[i-1,]#
		#STEP 2: Proximal step.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-3) < tol ){#
			converged=1;#
			break;#
		}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i))#
} #end function#
#
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #Use initial z value of zero.#
	s = rep(0,maxiter)	#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma*lambda)#
		#betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i,] + ((s[i-1]-1)/s[i]) * (betas[i,] - betas[i-1,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)#
#
#Compare results to glmnet:#
myLasso <- glmnet(X,y,family='gaussian',lambda=lam)	#fit glmnet model.#
beta_glmnet <- myLasso$beta							#Save glmnet betas.#
cbind(glmnet=beta_glmnet,#
	proximal=output$beta_hat,#
	accel.prox=round(outputAccel$beta_hat,8)) 	#output comparison#
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
### SDS 385 - Exercises 06 - Proximal Gradient Descent for LASSO.#
#
#Jennifer Starling#
#7 October 2016#
#
rm(list=ls())	#Clean workspace.#
#
library(glmnet)#
library(Matrix)#
#
#Read in Diabetes.csv data.#
X <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesX.csv',header=T)#
y <- read.csv(file='/Users/jennstarling/UTAustin/2016_Fall_SDS 385_Big_Data/Exercise 05 R Code/DiabetesY.csv',header=F)#
#
#Scale X and y.#
X = scale(X)#
y = scale(y)#
#
#----------------------------------------------#
#LASSO objective function:#
#Inputs:#
#	X = X matrix (scaled)#
#	y = response data (scaled)#
#	lambda = a chosen lambda value#
#	beta = a vector of beta coefficients.#
#Output:#
#	Value of the LASSO objective function at specified inputs.#
fx <- function(X,y,lambda,beta){#
	f = (1/nrow(X)) * (t(y - X %*% beta) %*% (y - X %*% beta))#
	g = lambda * sum(abs(beta))#
	obj = (f+g)#
	return(as.numeric(obj))#
}#
#
#Test:#
fx(X,y,lam,beta_glmnet)#
#
#----------------------------------------------#
#Proximal L1 Operator function: (soft thresholding operator)#
#Inputs:#
#	x = vector of values.#
#	lambda = the scaling factor of the l1 norm.#
#	t = the step size.#
#
#Output:#
#	Value of the soft-thresholding proximal operator.#
#
#prox_l1 <- function(x,gamma,tau=1) {#
#	thresh <- gamma*tau#
#	prox = rep(0,length(x))#
#	idx.1 = which(x < -thresh)#
#	idx.2 = which(x > thresh)#
#	idx.3 = which(abs(x) <= thresh)#
#	if (length(idx.1) > 0) prox[idx.1] = x[idx.1] + thresh#
#	if (length(idx.2) > 0) prox[idx.2] = x[idx.2] - thresh#
#	if (length(idx.3) > 0) prox[idx.3] = 0#
#
 #   return(prox)#
#}#
prox_l1 <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
  # ----------------------------------------#
  # Args: #
  #   - x: vector of the observations#
  #   - lambda: penalization parameter (threshold)#
  # Returns: #
  #   - theta: the soft thresholding estimator#
  # ------------------------------------------#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
  return (theta)#
}#
#
#----------------------------------------------#
#Gradient for differentiable (non-penalty) part of LASSO objective:#
gradient <- function(X,y,beta){#
	grad = (2/nrow(X)) * (t(X) %*% X %*% beta - t(X) %*% y )#
	return(grad)#
}	#
#
#----------------------------------------------#
#Proximal Gradient Descent for L1 Norm Function:#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
proxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) 		#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		#grad[i-1,] = (2/nrow(X)) * (t(X) %*% X %*% betas[i-1,]  - t(X) %*% y )#
		grad[i-1,] = gradient(X,y,betas[i-1,])#
		#Determine intermediate point.#
		z = betas[i-1,] - gamma*grad[i-1,]#
		#STEP 2: Proximal step.#
		betas[i,] = prox_l1(z,gamma*lambda)#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-3) < tol ){#
			converged=1;#
			break;#
		}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i))#
} #end function#
#
#----------------------------------------------#
#Accelerated Proximal Gradient Descent for L1 Norm Function:#
#(Nesterov)#
#Inputs:#
#	X = design matrix#
#	y = response vector#
#	gamma = step size#
#	maxiter = maximum iterations#
#	tol = tolerance for convergence#
#	lambda = l1 norm penalty constant.#
#Output:#
#	List including estimated beta values and objective function.#
#
accelProxGD <- function(X,Y,gamma=.01,maxiter=50,tol=1E-10,lambda=.1){#
	i=0					#Initialize iterator.#
	converged <- 0		#Indicator for whether convergence met.#
	#1. Initialize matrix to hold beta vector for each iteration.#
	betas <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	betas[1,] <- rep(0,ncol(X))	#Initialize beta vector to 0 to start.#
	#2. Initialize values for objective function.#
	obj <- rep(0,maxiter) 	#Initialize vector to hold loglikelihood fctn.#
	obj[1] <- fx(X,y,lambda,betas[1,])#
	#3. Initialize matrix to hold gradients for each iteration.					#
	grad <- matrix(0,nrow=maxiter,ncol=ncol(X)) #
	grad[1,] =  gradient(X,y,betas[1,])#
	#4. Initialize vectors to hold Nesterov update values.#
	z = matrix(0,nrow=maxiter,ncol=ncol(X)) #Use initial z value of zero.#
	s = rep(0,maxiter)	#
	#Set up scalar s terms.  Ok before main loop, as do not depend on other terms' updates.#
	for (j in 2:maxiter){#
		s[j] = (1 + sqrt(1 + 4*(s[j-1])^2)) / 2#
	}	#
	#Loop through iterations until converged or maxiter met.#
	for (i in 2:maxiter){#
		#STEP 1: Gradient Step.#
		#Calc gradient.#
		grad[i-1,] =  gradient(X,y,z[i-1,])#
		#Update intermediate u term.#
		u = z[i-1,] - gamma * grad[i-1,]#
		#STEP 2: Proximal step; update betas.#
		betas[i,] = prox_l1(u,gamma*lambda)#
		#betas[i,] = prox_l1(u,gamma,tau=lambda)#
		#STEP 3: Nesterov step; update Nesterov momentum z.#
		z[i,] = betas[i,] + ((s[i-1]-1)/s[i]) * (betas[i,] - betas[i-1,])#
		#Update objective function.#
		obj[i] = fx(X,y,lambda=lambda,beta=betas[i,])#
		#Check if convergence met: If yes, exit loop.#
		#if (abs(obj[i]-obj[i-1])/abs(obj[i-1]+1E-10) < tol ){#
		#	converged=1;#
		#	break;#
		#}#
	} #end for loop#
	return(list(obj=obj, betas=betas, beta_hat=betas[i,], converged=converged, iter=i,s=s))#
} #end function#
#
#----------------------------------------------#
#
#Run proximal gradient descent & accelerated proximal gradient descent.#
lam=.01#
output <- proxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
outputAccel <- accelProxGD(X,y,gamma=.01,maxiter=1000,tol=1E-10,lambda=lam)#
#
#Iterations to convergence:#
print(output$iter)#
print(outputAccel$iter)#
print(output$converged)#
print(outputAccel$converged)#
#
#Compare results to glmnet:#
myLasso <- glmnet(X,y,family='gaussian',lambda=lam)	#fit glmnet model.#
beta_glmnet <- myLasso$beta							#Save glmnet betas.#
cbind(glmnet=beta_glmnet,#
	proximal=output$beta_hat,#
	accel.prox=round(outputAccel$beta_hat,8)) 	#output comparison#
#Plot objective function.#
plot(1:output$iter,output$obj[1:output$iter],type='l',log='xy',col='blue',xlab=paste('iter ',1,' to ',output$iter),#
	ylab='objective function')#
lines(1:outputAccel$iter,outputAccel$obj[1:outputAccel$iter],type='l',col='red',xlab=paste('iter ',1,' to ',outputAccel$iter),#
	ylab='objective function')
soft.thresholding <- function(x, lambda){#
#
  # Computes the soft thresholding estimator#
#
  # ----------------------------------------#
#
  # Args: #
#
  #   - x: vector of the observations#
#
  #   - lambda: penalization parameter (threshold)#
#
  # Returns: #
#
  #   - theta: the soft thresholding estimator#
#
  # ------------------------------------------#
#
  theta <- sign(x) * pmax(rep(0, length(x)), abs(x) - lambda)#
#
  return (theta)#
#
}
x = c(1:10)
soft.thresholding(x,.5)
#Parametric bootstrap for var(theta).#
B <- 1000#
mu_boot <- rep(0,B)#
theta_boot <- rep(0,B)	#Initialize vector to hold bootstrap theta estimates.#
#
mu = 5#
sigma = 1#
n = 100#
#
th_hat <- exp(mean(rnorm(n,mu,sigma)))#
#
for (i in 1:B){#
	#Draw a sample from the distribution, since parametric.#
	y_boot <- rnorm(n,mu,sigma)#
	#Calculate statistics from the bootstrap draw.#
	mu_boot[i] <- mean(y_boot)#
	theta_boot[i] <- exp(mu_boot[i])#
}#
#
#Display bootstrapped variance.#
var_theta_boot <- var(theta_boot)#
var_theta_boot#
#
#Display 95% CI: (Using percentile interval)#
quantile(theta_boot,c(.025,.975))#
#
#Display 95% CI: (Using normal interval, for fun.  This interval is wider.)#
th_hat + c(-1,1) * 1.96 * sqrt(var_theta_boot)
th_hat
n=5
mat = matrix(rnorm(n*n),nrow=n)#
		mat[lower.tri(mat)] <- 0#
		diag(mat) <- 1
mat
#----------------------------------------------------------------#
#
vecAngle <- function(x,y) acos(sum(x %*% y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)) ))
x = c(1,2,3)
y = c(4,5,6)
vecAngle(x,y)
sum(x %*% y)
(sqrt(sum(x^2)) * sqrt(sum(y^2)) )
acos(sum(x %*% y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)) ))
acos(0.97463184619707621)
#Dirichlet Log-likelihood function:#
dirichlet.loglik <- function(data,alpha){	#
	N <- nrow(data)						#Number of (j) observations.#
	logxi <- colMeans(log(data))		#logxi = (1/N)*sum(log(x_ij)#
	logl <- N* (lgamma(sum(alpha)) - sum(lgamma(alpha)) + sum((alpha-1)*logxi))#
#
	return(logl)	#
}
x = c(1,2,3,4,5)
alpha = c(.25,.25,.25,.25)
dirichlet.loglik(x,alpha)
x = matrix(rnorm(4*5),nrow=5,ncol=4)
x
alpha = c(.2,.3,.4,.1)
sum(alpha)
dirichlet.loglik(x,alpha)
x = abs(x)
x
dirichlet.loglik(x,alpha)
?lgamma
linear.regression <- function(X,y){#
#
	n = length(y)#
	X = as.matrix(cbind(rep(1,n),X))#
	beta_hat <- solve(t(X) %*% X) %*% t(X) %*% y#
#
	#Calculate y_hat and RSS.#
	y_hat <- X %*% beta_hat#
	RSS <- t(y - X %*% beta_hat) %*% (y - X %*% beta_hat)#
#
	n <- nrow(X)	#Number of obs.#
	p <- 4			#Number of predictors.#
	sigma2_hat <- RSS / (n-p-1)		#Estimate of sigma^2.#
	sigma_hat <- sqrt(sigma2_hat)	#Estimate of sigma.#
#
	#Estimated standard error and CI for each coefficient:#
	beta_hat_se <- sqrt( sigma2_hat * diag(solve(t(X) %*% X)) ) #Standard errors.#
#
	#95% CI for beta_hat's.#
	lb <- beta_hat -  1.96 * beta_hat_se#
	ub <- beta_hat + 1.96 * beta_hat_se#
	sig <- ifelse( (lb < 0) & (ub > 0), 0,1)#
#
	#Display CI, including star for betas not equal to zero based on the CI.#
	ci = data.frame(lower.bound=lb,upper.bound=ub,sig.coefs=sig)#
	return(list(beta_hat = beta_hat, #
				RSS = RSS, #
				sigma_hat = sigma_hat, #
				beta_hat_se = beta_hat_se, #
				ci = ci,#
				n=n,#
				p = ncol(X)-1))#
} #End myLinearReg function.
